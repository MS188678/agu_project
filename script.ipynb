{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaeLfvgsDEXR",
        "outputId": "24f73fb7-221a-4356-a128-c18095ec999c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.4.26)\n",
            "Dataset loading...\n",
            "Dataset saved: /kaggle/input/twitter-and-reddit-sentimental-analysis-dataset\n",
            "Examples:\n",
            "                                          clean_text  category\n",
            "0  when modi promised “minimum government maximum...      -1.0\n",
            "1  talk all the nonsense and continue all the dra...       0.0\n",
            "2  what did just say vote for modi  welcome bjp t...       1.0\n",
            "3  asking his supporters prefix chowkidar their n...       1.0\n",
            "4  answer who among these the most powerful world...       1.0\n",
            "Dict. size: 29458\n",
            "Device:  cpu\n",
            "Model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:  84%|████████▍ | 2030/2420 [04:19<00:44,  8.80it/s, loss=0.311]"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub tqdm matplotlib\n",
        "\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "print(\"Dataset loading...\")\n",
        "path = kagglehub.dataset_download(\"cosmos98/twitter-and-reddit-sentimental-analysis-dataset\")\n",
        "print(\"Dataset saved:\", path)\n",
        "\n",
        "csv_file = os.path.join(path, \"Twitter_Data.csv\")\n",
        "assert os.path.exists(csv_file), \"File not found!\"\n",
        "\n",
        "df = pd.read_csv(csv_file)\n",
        "df = df.dropna()\n",
        "print(\"Examples:\")\n",
        "print(df.head())\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def map_label(label):\n",
        "    return { -1: \"negative\", 0: \"neutral\", 1: \"positive\" }.get(label, \"neutral\")\n",
        "\n",
        "df[\"label\"] = df[\"category\"].apply(map_label)\n",
        "\n",
        "MAX_LEN = 64\n",
        "\n",
        "def tokenize(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "counter = Counter()\n",
        "for text in df[\"clean_text\"]:\n",
        "    counter.update(tokenize(text))\n",
        "\n",
        "vocab = [\"<PAD>\", \"<UNK>\"] + [word for word, freq in counter.items() if freq >= 3]\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "vocab_size = len(word2idx)\n",
        "print(f\"Dict. size: {vocab_size}\")\n",
        "\n",
        "def encode(text):\n",
        "    tokens = tokenize(text)\n",
        "    idxs = [word2idx.get(token, word2idx[\"<UNK>\"]) for token in tokens]\n",
        "    idxs = idxs[:MAX_LEN] + [word2idx[\"<PAD>\"]] * (MAX_LEN - len(idxs))\n",
        "    return idxs\n",
        "\n",
        "label2class = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df[\"clean_text\"], df[\"label\"], test_size=0.05, random_state=42, stratify=df[\"label\"]\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.X = [encode(text) for text in texts]\n",
        "        self.y = [label2class[label] for label in labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = SentimentDataset(train_texts, train_labels)\n",
        "test_dataset = SentimentDataset(test_texts, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "    return torch.matmul(attn, v), attn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        q = self.q_linear(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_linear(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_linear(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        attn_output, _ = scaled_dot_product_attention(q, k, v, mask)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return self.dropout(self.out_linear(attn_output))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, ff_dim, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.norm1(x + self.dropout(self.self_attn(x, mask)))\n",
        "        return self.norm2(x + self.dropout(self.ff(x)))\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, n_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "class TransformerSentimentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, n_heads=4, ff_dim=128, num_layers=2, num_classes=3, max_len=MAX_LEN, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "        self.encoder = TransformerEncoder(num_layers, d_model, n_heads, ff_dim, dropout)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.encoder(x, mask)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.fc(self.dropout(x))\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "model = TransformerSentimentClassifier(vocab_size=vocab_size).to(device)\n",
        "\n",
        "READ_MODEL = True\n",
        "MODEL_PATH = \"/content/transformer_sentiment_model.pt\"\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "train_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "if READ_MODEL and os.path.exists(MODEL_PATH):\n",
        "    print(\"Loading model...\")\n",
        "    model.load_state_dict(torch.load(MODEL_PATH))\n",
        "else:\n",
        "    print(\"Model training...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "        for X_batch, y_batch in progress_bar:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1} Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                preds = torch.argmax(model(X_batch), dim=1)\n",
        "                correct += (preds == y_batch).sum().item()\n",
        "                total += y_batch.size(0)\n",
        "        acc = correct / total\n",
        "        test_accuracies.append(acc)\n",
        "        print(f\"📈 Test accuracy: {acc:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), MODEL_PATH)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss vs Epoch\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(test_accuracies, label=\"Test Accuracy\", color=\"green\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs Epoch\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        preds = torch.argmax(model(X_batch), dim=1).cpu().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(y_batch.tolist())\n",
        "\n",
        "print(\"\\n Raport klasyfikacji:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
        "\n",
        "custom_sentences = [\n",
        "    \"I absolutely loved the experience!\",\n",
        "    \"That was the best movie I've seen this year.\",\n",
        "    \"The customer service was outstanding.\",\n",
        "    \"I would definitely recommend this to my friends.\",\n",
        "    \"Everything worked perfectly, thank you!\",\n",
        "    \"Such a pleasant surprise!\",\n",
        "    \"This product exceeded my expectations.\",\n",
        "    \"I'm really happy with how things turned out.\",\n",
        "    \"Amazing quality and fast shipping!\",\n",
        "    \"The staff were super friendly and helpful.\",\n",
        "\n",
        "    \"This is the worst thing I've ever bought.\",\n",
        "    \"Completely disappointed with the outcome.\",\n",
        "    \"The experience was a total letdown.\",\n",
        "    \"It broke after one use — horrible.\",\n",
        "    \"Terrible customer support, nobody responded.\",\n",
        "    \"Totally not worth the money.\",\n",
        "    \"I'm so frustrated with this situation.\",\n",
        "    \"Unacceptable behavior from the staff.\",\n",
        "    \"Nothing worked as expected.\",\n",
        "    \"I regret wasting my time on this.\",\n",
        "\n",
        "    \"I'm still waiting to see how it turns out.\",\n",
        "    \"It was okay, nothing special.\",\n",
        "    \"I don't have strong feelings about this.\",\n",
        "    \"This might be useful for some people.\",\n",
        "    \"The process was straightforward.\",\n",
        "    \"I'll need more time to form an opinion.\",\n",
        "    \"Not good, not bad — just average.\",\n",
        "    \"It functions as described.\",\n",
        "    \"I tried it once, haven't used it since.\",\n",
        "    \"I guess it's fine for the price.\",\n",
        "]\n",
        "\n",
        "print(\"\\n Custom tests:\")\n",
        "with torch.no_grad():\n",
        "    for sentence in custom_sentences:\n",
        "        encoded = torch.tensor([encode(sentence)], dtype=torch.long).to(device)\n",
        "        output = model(encoded)\n",
        "        pred_class = torch.argmax(output, dim=1).item()\n",
        "        print(f\"📝 '{sentence}' → Predicted: {['negative', 'neutral', 'positive'][pred_class]}\")\n"
      ]
    }
  ]
}